#!/bin/bash
#
# SOAR Deployment Script
#
# This script is executed with sudo privileges to deploy SOAR updates.
# It must be installed at /usr/local/bin/soar-deploy with permissions 755.
#
# Usage:
#   sudo /usr/local/bin/soar-deploy [production|staging] /tmp/soar/deploy/YYYYMMDDHHMMSS
#   sudo /usr/local/bin/soar-deploy /tmp/soar/deploy/YYYYMMDDHHMMSS  (defaults to production)
#
# The deployment directory should contain:
#   - soar (the binary)
#   - soar-deploy (this script - for self-update)
#   - *.service files (production) or *-staging.service files (staging)
#   - *.timer files (production) or *-staging.timer files (staging)
#   - *.target files (soar@production.target or soar@staging.target)
#   - prometheus-jobs/ directory (optional, contains Prometheus job configs)
#   - grafana-provisioning/ directory (optional, contains Grafana provisioning configs)
#   - grafana-dashboard-*.json files (optional, Grafana dashboards)
#
# Self-Update:
#   This script will automatically update itself if the version in the deployment
#   directory differs from /usr/local/bin/soar-deploy, then re-execute with the
#   same arguments to ensure the latest deployment logic is used.
#

set -euo pipefail

# Deployment log and status directory
DEPLOY_LOG_DIR="/var/lib/soar/deploy/logs"
DEPLOY_STATUS_DIR="/var/lib/soar/deploy/status"

# Color codes for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
NC='\033[0m' # No Color

log_info() {
    echo -e "${GREEN}[INFO]${NC} $1"
}

log_warn() {
    echo -e "${YELLOW}[WARN]${NC} $1"
}

log_error() {
    echo -e "${RED}[ERROR]${NC} $1"
}

# Parse arguments: environment and deploy directory
# Supports both: "soar-deploy staging /path" and "soar-deploy /path" (defaults to production)
if [ $# -eq 2 ]; then
    ENVIRONMENT="$1"
    DEPLOY_DIR="$2"
elif [ $# -eq 1 ]; then
    ENVIRONMENT="production"
    DEPLOY_DIR="$1"
else
    log_error "Usage: $0 [production|staging] /tmp/soar/deploy/YYYYMMDDHHMMSS"
    log_error "   or: $0 /tmp/soar/deploy/YYYYMMDDHHMMSS  (defaults to production)"
    exit 1
fi

# Validate environment
if [ "$ENVIRONMENT" != "production" ] && [ "$ENVIRONMENT" != "staging" ]; then
    log_error "Invalid environment: $ENVIRONMENT (must be 'production' or 'staging')"
    exit 1
fi

# Verify deployment directory exists
if [ ! -d "$DEPLOY_DIR" ]; then
    log_error "Deployment directory does not exist: $DEPLOY_DIR"
    exit 1
fi

# Create log and status directories (this script runs as root via sudo)
mkdir -p "$DEPLOY_LOG_DIR"
mkdir -p "$DEPLOY_STATUS_DIR"
chown -R soar:soar /var/lib/soar/deploy

# Generate a unique deployment ID based on the deploy directory name and environment
# This allows the caller to predict the status file path
DEPLOY_DIR_BASENAME=$(basename "$DEPLOY_DIR")
DEPLOY_ID="${ENVIRONMENT}-${DEPLOY_DIR_BASENAME}"
STATUS_FILE="$DEPLOY_STATUS_DIR/$DEPLOY_ID.status"
LOG_FILE="$DEPLOY_LOG_DIR/$DEPLOY_ID.log"

# Print log file location BEFORE redirecting (so caller knows where to look)
echo "Deployment log file: $LOG_FILE"

# Redirect all output to log file using exec
# This allows the script to be run detached while still capturing all output
exec > "$LOG_FILE" 2>&1

# Write status file on exit (success or failure)
write_status() {
    local exit_code=$?
    echo "$exit_code" > "$STATUS_FILE"
    if [ $exit_code -eq 0 ]; then
        log_info "Deployment completed successfully (exit code: $exit_code)"
    else
        log_error "Deployment failed (exit code: $exit_code)"
    fi
    log_info "Status written to: $STATUS_FILE"
}
trap write_status EXIT

log_info "Deployment ID: $DEPLOY_ID"
log_info "Status file: $STATUS_FILE"
log_info "Starting SOAR deployment for $ENVIRONMENT from: $DEPLOY_DIR"

# Self-update: Check if deployment script itself has changed
if [ -f "$DEPLOY_DIR/soar-deploy" ]; then
    if ! diff -q "$0" "$DEPLOY_DIR/soar-deploy" >/dev/null 2>&1; then
        log_warn "Deployment script has changed - updating /usr/local/bin/soar-deploy and re-executing..."

        # Install updated deployment script
        install -m 755 -o root -g root "$DEPLOY_DIR/soar-deploy" /usr/local/bin/soar-deploy

        log_info "Deployment script updated, re-executing with same arguments..."
        # Re-exec the updated script with original arguments
        exec /usr/local/bin/soar-deploy "$@"
    else
        log_info "Deployment script is up-to-date"
    fi
else
    log_warn "No soar-deploy found in deployment directory, skipping self-update check"
fi

# Set environment-specific variables
if [ "$ENVIRONMENT" = "staging" ]; then
    SERVICE_SUFFIX="-staging"
    ENV_FILE="/etc/soar/env-staging"
    BINARY_PATH="/usr/local/bin/soar-staging"
    BACKUP_DIR="/home/soar/backups/staging"
    TARGET_FILE="soar@staging.target"
else
    SERVICE_SUFFIX=""
    ENV_FILE="/etc/soar/env-production"
    BINARY_PATH="/usr/local/bin/soar"
    BACKUP_DIR="/home/soar/backups/production"
    TARGET_FILE="soar@production.target"
fi

log_info "Environment: $ENVIRONMENT"
log_info "Service suffix: '$SERVICE_SUFFIX'"
log_info "Environment file: $ENV_FILE"
log_info "Binary path: $BINARY_PATH"

# Verify binary exists
if [ ! -f "$DEPLOY_DIR/soar" ]; then
    log_error "Binary not found in deployment directory: $DEPLOY_DIR/soar"
    exit 1
fi

# Define services and timers based on environment
# Active services that should be enabled and started
ACTIVE_SERVICES=(
    "soar-run${SERVICE_SUFFIX}.service"
    "soar-web${SERVICE_SUFFIX}.service"
    "soar-ingest${SERVICE_SUFFIX}.service"
)

# Manually-deployed services (installed but not automatically started)
# These services change infrequently and are deployed via the regular deployment process
MANUAL_SERVICES=(
)

# Timer-invoked services (only installed, not enabled or started)
# Skip backup services for staging environment
if [ "$ENVIRONMENT" = "staging" ]; then
    TIMER_SERVICES=(
        "soar-pull-data${SERVICE_SUFFIX}.service"
        "soar-sitemap${SERVICE_SUFFIX}.service"
        "soar-archive${SERVICE_SUFFIX}.service"
        "soar-run-aggregates${SERVICE_SUFFIX}.service"
    )
    TIMERS=(
        "soar-pull-data${SERVICE_SUFFIX}.timer"
        "soar-sitemap${SERVICE_SUFFIX}.timer"
        "soar-archive${SERVICE_SUFFIX}.timer"
        "soar-run-aggregates${SERVICE_SUFFIX}.timer"
    )
else
    TIMER_SERVICES=(
        "soar-pull-data${SERVICE_SUFFIX}.service"
        "soar-sitemap${SERVICE_SUFFIX}.service"
        "soar-archive${SERVICE_SUFFIX}.service"
        "soar-run-aggregates${SERVICE_SUFFIX}.service"
        "soar-backup-base${SERVICE_SUFFIX}.service"
        "soar-backup-verify${SERVICE_SUFFIX}.service"
    )
    TIMERS=(
        "soar-pull-data${SERVICE_SUFFIX}.timer"
        "soar-sitemap${SERVICE_SUFFIX}.timer"
        "soar-archive${SERVICE_SUFFIX}.timer"
        "soar-run-aggregates${SERVICE_SUFFIX}.timer"
        "soar-backup-base${SERVICE_SUFFIX}.timer"
        "soar-backup-verify${SERVICE_SUFFIX}.timer"
    )
fi

# Template services (no environment suffix, used with systemd @instance syntax)
TEMPLATE_SERVICES=(
    "soar-migrate@.service"
)

# All services (for stopping and installing)
ALL_SERVICES=("${ACTIVE_SERVICES[@]}" "${MANUAL_SERVICES[@]}" "${TIMER_SERVICES[@]}" "${TEMPLATE_SERVICES[@]}")

# Create temporary binary directory with timestamp
TIMESTAMP=$(date +%s)
TEMP_BIN_DIR="/var/lib/soar/bin"
TEMP_BIN_PATH="$TEMP_BIN_DIR/soar-$TIMESTAMP"

log_info "Creating temporary binary directory: $TEMP_BIN_DIR"
mkdir -p "$TEMP_BIN_DIR"
chown soar:soar "$TEMP_BIN_DIR"

# Deploy new binary to temporary location first
log_info "Deploying new binary to temporary location: $TEMP_BIN_PATH"
install -m 755 -o soar -g soar "$DEPLOY_DIR/soar" "$TEMP_BIN_PATH"
log_info "Binary deployed to temporary location"

# Backup current binary and install new binary IMMEDIATELY
# This ensures migrations run with the NEW binary, not the old one
# (Running processes keep using the old inode, new processes get the new binary)
mkdir -p "$BACKUP_DIR"
chown soar:soar "$BACKUP_DIR"

if [ -f "$BINARY_PATH" ]; then
    BACKUP_FILE="$BACKUP_DIR/soar.backup.$(date +%s)"
    log_info "Backing up current binary to $BACKUP_FILE..."
    cp "$BINARY_PATH" "$BACKUP_FILE"
    chown soar:soar "$BACKUP_FILE"
else
    log_warn "No existing binary to backup at $BINARY_PATH"
fi

log_info "Installing new binary to $BINARY_PATH..."
install -m 755 -o root -g root "$TEMP_BIN_PATH" "$BINARY_PATH"
log_info "Binary installed successfully at $BINARY_PATH"

# Clean up temporary binary
log_info "Cleaning up temporary binary: $TEMP_BIN_PATH"
rm -f "$TEMP_BIN_PATH"

# Update SENTRY_RELEASE in environment file from VERSION file
if [ -f "$DEPLOY_DIR/VERSION" ]; then
    VERSION=$(cat "$DEPLOY_DIR/VERSION" | tr -d '\n' | tr -d '\r')
    log_info "Setting SENTRY_RELEASE=$VERSION in $ENV_FILE..."

    # Create environment file if it doesn't exist
    if [ ! -f "$ENV_FILE" ]; then
        log_warn "Environment file $ENV_FILE not found, creating it..."
        touch "$ENV_FILE"
        chown soar:soar "$ENV_FILE"
        chmod 640 "$ENV_FILE"
    fi

    # Update or add SENTRY_RELEASE line
    if grep -q "^SENTRY_RELEASE=" "$ENV_FILE"; then
        # Replace existing SENTRY_RELEASE line
        sed -i "s|^SENTRY_RELEASE=.*|SENTRY_RELEASE=$VERSION|" "$ENV_FILE"
        log_info "Updated existing SENTRY_RELEASE to $VERSION"
    else
        # Add new SENTRY_RELEASE line
        echo "SENTRY_RELEASE=$VERSION" >> "$ENV_FILE"
        log_info "Added SENTRY_RELEASE=$VERSION to environment file"
    fi
else
    log_warn "VERSION file not found in deployment directory, skipping SENTRY_RELEASE configuration"
    log_warn "Sentry release tracking may not work correctly"
fi

# Install migration service template early (needed before running migrations)
log_info "Installing migration service template..."
for service in "${TEMPLATE_SERVICES[@]}"; do
    if [ -f "$DEPLOY_DIR/$service" ]; then
        log_info "Installing $service..."
        cp "$DEPLOY_DIR/$service" /etc/systemd/system/
        chmod 644 "/etc/systemd/system/$service"
    else
        log_warn "$service not found in deployment directory, skipping"
    fi
done

# Reload systemd daemon to pick up new migration service
log_info "Reloading systemd daemon..."
systemctl daemon-reload

# Stop soar-run service only (for migrations, keep others running)
log_info "Stopping soar-run${SERVICE_SUFFIX} service for migrations..."
if systemctl is-active --quiet "soar-run${SERVICE_SUFFIX}.service"; then
    systemctl stop "soar-run${SERVICE_SUFFIX}.service" || log_warn "Failed to stop soar-run${SERVICE_SUFFIX}.service"
    log_info "soar-run${SERVICE_SUFFIX}.service stopped"
else
    log_info "soar-run${SERVICE_SUFFIX}.service is not running"
fi

# Wait for service to fully terminate
sleep 2

# Check current migration status before running migrations
log_info "Checking current database migration status..."
sudo -u soar bash -c "set -a; source $ENV_FILE; set +a; psql \"\$DATABASE_URL\" -c \"SELECT version FROM __diesel_schema_migrations ORDER BY version DESC LIMIT 5;\"" 2>/dev/null || log_warn "Could not query migration status"

# Run database migrations via systemd service (async, survives SSH disconnect)
log_info "Starting database migrations via systemd service..."
log_info "This allows migrations to continue if SSH connection is lost"

# Reset the migration service to clear any previous state
systemctl reset-failed "soar-migrate@${ENVIRONMENT}.service" 2>/dev/null || true

# Start the migration service (runs asynchronously)
if ! systemctl start "soar-migrate@${ENVIRONMENT}.service"; then
    log_error "Failed to start migration service"
    log_error "Check: systemctl status soar-migrate@${ENVIRONMENT}.service"
    log_info "Cleaning up temporary binary: $TEMP_BIN_PATH"
    rm -f "$TEMP_BIN_PATH"
    exit 1
fi

log_info "Migration service started successfully"
log_info "Monitoring migration progress..."

# Poll for migration completion (check every 5 seconds, timeout after 2 hours)
MAX_WAIT=7200  # 2 hours
ELAPSED=0
POLL_INTERVAL=5

while [ $ELAPSED -lt $MAX_WAIT ]; do
    # Check systemd service status
    if systemctl is-active --quiet "soar-migrate@${ENVIRONMENT}.service"; then
        # Still running
        echo -n "."
        sleep $POLL_INTERVAL
        ELAPSED=$((ELAPSED + POLL_INTERVAL))

        # Show progress every 30 seconds
        if [ $((ELAPSED % 30)) -eq 0 ]; then
            log_info "Migration still running... (${ELAPSED}s elapsed)"
            # Show last few log lines
            journalctl -u "soar-migrate@${ENVIRONMENT}.service" -n 3 --no-pager 2>/dev/null || true
        fi
    elif systemctl is-failed --quiet "soar-migrate@${ENVIRONMENT}.service"; then
        # Migration failed
        echo ""
        log_error "Database migration FAILED"
        log_error "Migration service exited with error"
        log_error "Database may be in a partially migrated state"
        log_error "DO NOT restart services - manual intervention required"
        echo ""
        log_error "Last 50 lines of migration logs:"
        journalctl -u "soar-migrate@${ENVIRONMENT}.service" -n 50 --no-pager
        echo ""
        log_info "Cleaning up temporary binary: $TEMP_BIN_PATH"
        rm -f "$TEMP_BIN_PATH"
        log_error "Deployment aborted. soar-run${SERVICE_SUFFIX}.service remains stopped for safety."
        log_info "To investigate:"
        log_info "  1. Check migration logs: journalctl -u soar-migrate@${ENVIRONMENT} -n 200"
        log_info "  2. Check systemctl status: systemctl status soar-migrate@${ENVIRONMENT}"
        log_info "  3. Verify database state manually"
        log_info "  4. Fix any issues before attempting to restart services"
        log_error "NOTE: Email and Sentry notifications were sent automatically by soar migrate"
        exit 1
    else
        # Migration completed successfully
        echo ""
        log_info "Database migrations completed successfully (${ELAPSED}s)"
        log_info "Email notification sent by soar migrate command"

        break
    fi
done

# Check if we timed out
if [ $ELAPSED -ge $MAX_WAIT ]; then
    log_error "Migration timed out after ${MAX_WAIT} seconds"
    log_error "Migration may still be running in background"
    log_error "Check: systemctl status soar-migrate@${ENVIRONMENT}.service"
    log_info "Cleaning up temporary binary: $TEMP_BIN_PATH"
    rm -f "$TEMP_BIN_PATH"
    exit 1
fi

# Track which manual services were running before stopping them
# We'll restart these after deployment completes successfully
MANUAL_SERVICES_TO_RESTART=()
for service in "${MANUAL_SERVICES[@]}"; do
    if systemctl is-active --quiet "$service"; then
        log_info "Recording $service as running (will restart after deployment)"
        MANUAL_SERVICES_TO_RESTART+=("$service")
    fi
done

# Now stop ALL remaining services (migrations are complete)
log_info "Stopping remaining SOAR services..."
for service in "${ALL_SERVICES[@]}"; do
    # Skip soar-run since we already stopped it
    if [ "$service" = "soar-run${SERVICE_SUFFIX}.service" ]; then
        continue
    fi

    if systemctl is-active --quiet "$service"; then
        log_info "Stopping $service..."
        systemctl stop "$service" || log_warn "Failed to stop $service"
    else
        log_info "$service is not running"
    fi
done

# Note: We intentionally do NOT stop timers during deployment
# Timers pick up configuration changes via daemon-reload without needing to be stopped/started
# Stopping and restarting timers with Persistent=yes can trigger immediate runs
log_info "Skipping timer stop (timers will continue on their schedule)"

# Wait for services to fully terminate
log_info "Waiting for services to fully terminate..."
sleep 3

# Binary was already installed earlier (before migrations)
# Check for obsolete systemd files that should be cleaned up
log_info "Checking for obsolete systemd service and timer files..."
OBSOLETE_FILES_FOUND=0

# Build list of expected files from deployment
EXPECTED_FILES=()
for service in "${ALL_SERVICES[@]}"; do
    EXPECTED_FILES+=("$service")
done
for timer in "${TIMERS[@]}"; do
    EXPECTED_FILES+=("$timer")
done
EXPECTED_FILES+=("$TARGET_FILE")

# Find all SOAR-related systemd files in /etc/systemd/system
ALL_SOAR_FILES=()
shopt -s nullglob  # Make globs that don't match expand to nothing instead of literal pattern
for pattern in "soar*.service" "soar*.timer" "soar*.target"; do
    for file in /etc/systemd/system/$pattern; do
        if [ -f "$file" ]; then
            ALL_SOAR_FILES+=("$(basename "$file")")
        fi
    done
done
shopt -u nullglob

# Check each found file against expected files
for file_basename in "${ALL_SOAR_FILES[@]}"; do
    # For staging environment, only check files with -staging suffix or @staging
    # For production, only check files without -staging suffix and not @staging
    if [ "$ENVIRONMENT" = "staging" ]; then
        if [[ ! "$file_basename" =~ -staging\.(service|timer)$ ]] && [[ ! "$file_basename" =~ @staging\.target$ ]]; then
            continue  # Skip production files when in staging mode
        fi
    else
        if [[ "$file_basename" =~ -staging\.(service|timer)$ ]] || [[ "$file_basename" =~ @staging\.target$ ]]; then
            continue  # Skip staging files when in production mode
        fi
    fi

    # Check if this file is expected (in our deployment)
    is_expected=false
    for expected_file in "${EXPECTED_FILES[@]}"; do
        if [ "$file_basename" = "$expected_file" ]; then
            is_expected=true
            break
        fi
    done

    # If not expected, warn about it but don't delete
    if [ "$is_expected" = false ]; then
        log_warn "Obsolete systemd file detected: $file_basename"
        log_warn "  This file may need manual cleanup. To remove it, run:"
        log_warn "  sudo systemctl stop $file_basename 2>/dev/null || true"
        log_warn "  sudo systemctl disable $file_basename 2>/dev/null || true"
        log_warn "  sudo rm -f /etc/systemd/system/$file_basename"
        ((OBSOLETE_FILES_FOUND++)) || true
    fi
done

if [ $OBSOLETE_FILES_FOUND -gt 0 ]; then
    log_warn "Found $OBSOLETE_FILES_FOUND obsolete systemd file(s) - manual cleanup recommended"
else
    log_info "No obsolete systemd files found"
fi

# Install service files (skip template services, they were installed earlier)
log_info "Installing service files..."
for service in "${ALL_SERVICES[@]}"; do
    # Skip template services (already installed before migrations)
    skip=false
    for template_service in "${TEMPLATE_SERVICES[@]}"; do
        if [ "$service" = "$template_service" ]; then
            skip=true
            break
        fi
    done

    if [ "$skip" = true ]; then
        log_info "$service already installed (template service)"
        continue
    fi

    if [ -f "$DEPLOY_DIR/$service" ]; then
        log_info "Installing $service..."
        cp "$DEPLOY_DIR/$service" /etc/systemd/system/
        chmod 644 "/etc/systemd/system/$service"
    else
        log_warn "$service not found in deployment directory, skipping"
    fi
done

# Install timer files
log_info "Installing timer files..."
for timer in "${TIMERS[@]}"; do
    if [ -f "$DEPLOY_DIR/$timer" ]; then
        log_info "Installing $timer..."
        cp "$DEPLOY_DIR/$timer" /etc/systemd/system/
        chmod 644 "/etc/systemd/system/$timer"
    else
        log_warn "$timer not found in deployment directory, skipping"
    fi
done

# Install target file
if [ -f "$DEPLOY_DIR/$TARGET_FILE" ]; then
    log_info "Installing $TARGET_FILE..."
    cp "$DEPLOY_DIR/$TARGET_FILE" /etc/systemd/system/
    chmod 644 "/etc/systemd/system/$TARGET_FILE"
else
    log_warn "$TARGET_FILE not found in deployment directory, skipping"
fi

# Install backup scripts
if [ -d "$DEPLOY_DIR/scripts/backup" ]; then
    log_info "Installing backup scripts to /usr/local/bin/..."

    # Install each backup script with soar- prefix
    for script in wal-archive base-backup backup-verify restore; do
        if [ -f "$DEPLOY_DIR/scripts/backup/$script" ]; then
            log_info "Installing soar-$script..."
            install -m 755 -o root -g root "$DEPLOY_DIR/scripts/backup/$script" "/usr/local/bin/soar-$script"
        else
            log_warn "scripts/backup/$script not found, skipping"
        fi
    done

    log_info "Backup scripts installed successfully"
else
    log_warn "scripts/backup directory not found in deployment, skipping backup script installation"
fi

# Install main Prometheus configuration file
PROMETHEUS_NEEDS_RELOAD=false
if [ -f "$DEPLOY_DIR/prometheus.yml" ]; then
    log_info "Installing main Prometheus configuration file..."

    # Check if config differs from current
    if [ -f "/etc/prometheus/prometheus.yml" ]; then
        if ! diff -q "$DEPLOY_DIR/prometheus.yml" "/etc/prometheus/prometheus.yml" > /dev/null 2>&1; then
            log_info "Prometheus config has changed, updating..."
            cp "$DEPLOY_DIR/prometheus.yml" /etc/prometheus/prometheus.yml
            chmod 644 /etc/prometheus/prometheus.yml
            if id "prometheus" &>/dev/null; then
                chown prometheus:prometheus /etc/prometheus/prometheus.yml
            fi
            PROMETHEUS_NEEDS_RELOAD=true
            log_info "Prometheus configuration file updated"
        else
            log_info "Prometheus config unchanged, skipping"
        fi
    else
        log_info "No existing Prometheus config, installing..."
        cp "$DEPLOY_DIR/prometheus.yml" /etc/prometheus/prometheus.yml
        chmod 644 /etc/prometheus/prometheus.yml
        if id "prometheus" &>/dev/null; then
            chown prometheus:prometheus /etc/prometheus/prometheus.yml
        fi
        PROMETHEUS_NEEDS_RELOAD=true
        log_info "Prometheus configuration file installed"
    fi
else
    log_warn "prometheus.yml not found in deployment, skipping main Prometheus config"
fi

# Install Prometheus job configuration files
if [ -d "$DEPLOY_DIR/prometheus-jobs" ]; then
    log_info "Installing Prometheus job configuration files..."
    mkdir -p /etc/prometheus/jobs

    # Remove old SOAR Prometheus job files for this environment
    if [ -d "/etc/prometheus/jobs" ]; then
        log_info "Cleaning up old Prometheus job files for $ENVIRONMENT..."
        for old_job in /etc/prometheus/jobs/soar-*.yml; do
            if [ -f "$old_job" ]; then
                job_basename=$(basename "$old_job")

                # Check if this file matches the current environment
                if [ "$ENVIRONMENT" = "staging" ]; then
                    # In staging, remove only staging files that aren't in deployment
                    if [[ "$job_basename" =~ -staging\.yml$ ]]; then
                        if [ ! -f "$DEPLOY_DIR/prometheus-jobs/$job_basename" ]; then
                            log_info "Removing obsolete Prometheus job: $job_basename"
                            rm -f "$old_job"
                        fi
                    fi
                else
                    # In production, remove only production files (no -staging suffix) that aren't in deployment
                    if [[ ! "$job_basename" =~ -staging\.yml$ ]]; then
                        if [ ! -f "$DEPLOY_DIR/prometheus-jobs/$job_basename" ]; then
                            log_info "Removing obsolete Prometheus job: $job_basename"
                            rm -f "$old_job"
                        fi
                    fi
                fi
            fi
        done
    fi

    # Copy only environment-appropriate Prometheus job configs
    if [ "$ENVIRONMENT" = "staging" ]; then
        log_info "Installing staging Prometheus job configs (*-staging.yml)..."
        for job_file in "$DEPLOY_DIR/prometheus-jobs"/*-staging.yml; do
            if [ -f "$job_file" ]; then
                cp "$job_file" /etc/prometheus/jobs/
                chmod 644 "/etc/prometheus/jobs/$(basename "$job_file")"
                log_info "  Installed: $(basename "$job_file")"
            fi
        done
    else
        log_info "Installing production Prometheus job configs (not *-staging.yml)..."
        for job_file in "$DEPLOY_DIR/prometheus-jobs"/*.yml; do
            if [ -f "$job_file" ]; then
                job_basename=$(basename "$job_file")
                # Skip staging files in production deployment
                if [[ ! "$job_basename" =~ -staging\.yml$ ]]; then
                    cp "$job_file" /etc/prometheus/jobs/
                    chmod 644 "/etc/prometheus/jobs/$job_basename"
                    log_info "  Installed: $job_basename"
                fi
            fi
        done
    fi

    # Set ownership to prometheus user if it exists
    if id "prometheus" &>/dev/null; then
        chown -R prometheus:prometheus /etc/prometheus/jobs
        log_info "Prometheus job files installed and ownership set to prometheus:prometheus"
        PROMETHEUS_NEEDS_RELOAD=true
    else
        log_warn "prometheus user not found, skipping ownership change"
        log_info "Prometheus job files installed"
    fi
else
    log_warn "prometheus-jobs directory not found in deployment, skipping Prometheus configuration"
fi

# Install Prometheus scrape-configs files (backward compatibility for old prometheus.yml)
if [ -d "$DEPLOY_DIR/prometheus-scrape-configs" ]; then
    log_info "Installing Prometheus scrape-configs files (legacy file_sd_configs)..."
    mkdir -p /etc/prometheus/scrape-configs

    # Remove old SOAR scrape-config files for this environment
    if [ -d "/etc/prometheus/scrape-configs" ]; then
        log_info "Cleaning up old scrape-config files for $ENVIRONMENT..."
        for old_config in /etc/prometheus/scrape-configs/soar-*.yml; do
            if [ -f "$old_config" ]; then
                config_basename=$(basename "$old_config")

                # Check if this file matches the current environment
                if [ "$ENVIRONMENT" = "staging" ]; then
                    # In staging, remove only staging files that aren't in deployment
                    if [[ "$config_basename" =~ -staging\.yml$ ]]; then
                        if [ ! -f "$DEPLOY_DIR/prometheus-scrape-configs/$config_basename" ]; then
                            log_info "Removing obsolete scrape-config: $config_basename"
                            rm -f "$old_config"
                        fi
                    fi
                else
                    # In production, remove only production files (no -staging suffix) that aren't in deployment
                    if [[ ! "$config_basename" =~ -staging\.yml$ ]]; then
                        if [ ! -f "$DEPLOY_DIR/prometheus-scrape-configs/$config_basename" ]; then
                            log_info "Removing obsolete scrape-config: $config_basename"
                            rm -f "$old_config"
                        fi
                    fi
                fi
            fi
        done
    fi

    # Copy only environment-appropriate Prometheus scrape configs
    if [ "$ENVIRONMENT" = "staging" ]; then
        log_info "Installing staging Prometheus scrape configs (*-staging.yml)..."
        for config_file in "$DEPLOY_DIR/prometheus-scrape-configs"/*-staging.yml; do
            if [ -f "$config_file" ]; then
                cp "$config_file" /etc/prometheus/scrape-configs/
                chmod 644 "/etc/prometheus/scrape-configs/$(basename "$config_file")"
                log_info "  Installed: $(basename "$config_file")"
            fi
        done
    else
        log_info "Installing production Prometheus scrape configs (not *-staging.yml)..."
        for config_file in "$DEPLOY_DIR/prometheus-scrape-configs"/*.yml; do
            if [ -f "$config_file" ]; then
                config_basename=$(basename "$config_file")
                # Skip staging files in production deployment and README
                if [[ ! "$config_basename" =~ -staging\.yml$ ]] && [[ ! "$config_basename" =~ README ]]; then
                    cp "$config_file" /etc/prometheus/scrape-configs/
                    chmod 644 "/etc/prometheus/scrape-configs/$config_basename"
                    log_info "  Installed: $config_basename"
                fi
            fi
        done
    fi

    # Set ownership to prometheus user if it exists
    if id "prometheus" &>/dev/null; then
        chown -R prometheus:prometheus /etc/prometheus/scrape-configs
        log_info "Prometheus scrape-config files installed and ownership set"
        PROMETHEUS_NEEDS_RELOAD=true
    else
        log_warn "prometheus user not found, skipping ownership change"
        log_info "Prometheus scrape-config files installed"
    fi
else
    log_warn "prometheus-scrape-configs directory not found in deployment, skipping legacy Prometheus configuration"
fi

# Reload Prometheus if job configurations were updated
if [ "$PROMETHEUS_NEEDS_RELOAD" = true ]; then
    if systemctl is-active --quiet prometheus; then
        log_info "Reloading Prometheus to load new scrape configurations..."
        systemctl reload prometheus || log_warn "Failed to reload Prometheus"
        log_info "Prometheus reloaded successfully"
    else
        log_warn "Prometheus is not running, skipping reload"
    fi
fi

# Install Grafana dashboards and provisioning configuration
GRAFANA_NEEDS_RESTART=false

if [ -d "$DEPLOY_DIR/grafana-provisioning" ]; then
    log_info "Installing Grafana provisioning configuration..."

    # Install datasource provisioning
    if [ -d "$DEPLOY_DIR/grafana-provisioning/datasources" ]; then
        log_info "Installing Grafana datasource configuration..."
        mkdir -p /etc/grafana/provisioning/datasources

        # Copy both .yml and .yaml files (Grafana datasources require .yaml extension)
        # Skip template files - they will be processed separately
        for datasource_file in "$DEPLOY_DIR/grafana-provisioning/datasources"/*.yml "$DEPLOY_DIR/grafana-provisioning/datasources"/*.yaml; do
            if [ -f "$datasource_file" ]; then
                filename=$(basename "$datasource_file")
                # Skip template files
                if [[ "$filename" == *.template ]]; then
                    continue
                fi
                cp "$datasource_file" /etc/grafana/provisioning/datasources/
                chmod 644 "/etc/grafana/provisioning/datasources/$filename"
                log_info "Installed datasource config: $filename"
            fi
        done

        # Process PostgreSQL datasource template if it exists
        if [ -f "$DEPLOY_DIR/grafana-provisioning/datasources/soar-postgres.yaml.template" ]; then
            log_info "Processing PostgreSQL datasource template..."

            # Set environment-specific values
            if [ "$ENVIRONMENT" = "staging" ]; then
                PG_DATABASE="soar_staging"
                PG_DATASOURCE_UID="soar-postgres-staging"
                PG_DATASOURCE_NAME="soar-postgres-staging"
            else
                PG_DATABASE="soar"
                PG_DATASOURCE_UID="soar-postgres"
                PG_DATASOURCE_NAME="soar-postgres"
            fi

            # Process template (no password needed - PgBouncer uses auth_type=any on localhost)
            sed -e "s|{{POSTGRES_DATABASE}}|$PG_DATABASE|g" \
                -e "s|{{POSTGRES_DATASOURCE_UID}}|$PG_DATASOURCE_UID|g" \
                -e "s|{{POSTGRES_DATASOURCE_NAME}}|$PG_DATASOURCE_NAME|g" \
                "$DEPLOY_DIR/grafana-provisioning/datasources/soar-postgres.yaml.template" \
                > /etc/grafana/provisioning/datasources/soar-postgres.yaml
            chmod 644 /etc/grafana/provisioning/datasources/soar-postgres.yaml
            log_info "PostgreSQL datasource configured: $PG_DATASOURCE_NAME -> $PG_DATABASE"
        fi

        # Set ownership to grafana user
        if id "grafana" &>/dev/null; then
            chown -R grafana:grafana /etc/grafana/provisioning/datasources
            log_info "Grafana datasource configuration installed successfully"
            GRAFANA_NEEDS_RESTART=true
        else
            log_warn "grafana user not found, skipping ownership change"
        fi
    else
        log_info "No datasource configuration found in deployment, skipping"
    fi

    # Install dashboard provisioning
    mkdir -p /etc/grafana/provisioning/dashboards
    cp -r "$DEPLOY_DIR/grafana-provisioning/dashboards"/*.yml /etc/grafana/provisioning/dashboards/
    chmod 644 /etc/grafana/provisioning/dashboards/*.yml

    # Set ownership to grafana user if it exists
    if id "grafana" &>/dev/null; then
        chown -R grafana:grafana /etc/grafana/provisioning/dashboards
        log_info "Grafana provisioning configuration installed"
        GRAFANA_NEEDS_RESTART=true
    else
        log_warn "grafana user not found, skipping ownership change"
    fi

    # Install Grafana alerting configuration with SMTP credentials substitution
    if [ -d "$DEPLOY_DIR/grafana-provisioning/alerting" ]; then
        log_info "Installing Grafana alerting configuration..."
        mkdir -p /etc/grafana/provisioning/alerting

        # Source environment file to get SMTP config and alert email
        if [ -f "$ENV_FILE" ]; then
            set -a
            source "$ENV_FILE"
            set +a

            # Extract variables with defaults
            ALERT_EMAIL="${EMAIL_TO:-ops@example.com}"
            SMTP_HOST="${SMTP_SERVER:-smtp.example.com}"
            SMTP_PORT_VAL="${SMTP_PORT:-587}"
            SMTP_USER="${SMTP_USERNAME:-noreply}"
            SMTP_PASS="${SMTP_PASSWORD:-}"
            FROM_ADDR="${FROM_EMAIL:-noreply@example.com}"
            FROM_NAME_VAL="${FROM_NAME:-SOAR System}"

            # Process contact-points.yml.template if it exists
            if [ -f "$DEPLOY_DIR/grafana-provisioning/alerting/contact-points.yml.template" ]; then
                log_info "Processing contact-points.yml.template with SMTP config from $ENV_FILE..."
                sed -e "s|{{ALERT_EMAIL}}|$ALERT_EMAIL|g" \
                    "$DEPLOY_DIR/grafana-provisioning/alerting/contact-points.yml.template" \
                    > /etc/grafana/provisioning/alerting/contact-points.yml
                chmod 640 /etc/grafana/provisioning/alerting/contact-points.yml
                log_info "Alert contact point configured for: $ALERT_EMAIL"
            fi

            # Copy other alerting config files as-is (no template processing needed)
            for alerting_file in "$DEPLOY_DIR/grafana-provisioning/alerting"/*.yml; do
                if [ -f "$alerting_file" ]; then
                    filename=$(basename "$alerting_file")
                    # Skip template files
                    if [[ "$filename" != *.template ]]; then
                        # Skip environment-specific alert rules that don't match current environment
                        if [[ "$filename" == "alert-rules-production.yml" && "$ENVIRONMENT" == "staging" ]]; then
                            log_info "Skipping $filename (production-only alerts)"
                            continue
                        fi
                        if [[ "$filename" == "alert-rules-staging.yml" && "$ENVIRONMENT" == "production" ]]; then
                            log_info "Skipping $filename (staging-only alerts)"
                            continue
                        fi

                        cp "$alerting_file" /etc/grafana/provisioning/alerting/
                        chmod 644 "/etc/grafana/provisioning/alerting/$filename"
                        log_info "Installed alerting config: $filename"
                    fi
                fi
            done

            # Process Grafana SMTP configuration via systemd drop-in
            if [ -f "$DEPLOY_DIR/systemd/grafana-server.service.d-smtp.conf.template" ]; then
                log_info "Configuring Grafana SMTP settings..."
                mkdir -p /etc/systemd/system/grafana-server.service.d

                sed -e "s|{{SMTP_SERVER}}|$SMTP_HOST|g" \
                    -e "s|{{SMTP_PORT}}|$SMTP_PORT_VAL|g" \
                    -e "s|{{SMTP_USERNAME}}|$SMTP_USER|g" \
                    -e "s|{{SMTP_PASSWORD}}|$SMTP_PASS|g" \
                    -e "s|{{FROM_EMAIL}}|$FROM_ADDR|g" \
                    -e "s|{{FROM_NAME}}|$FROM_NAME_VAL|g" \
                    "$DEPLOY_DIR/systemd/grafana-server.service.d-smtp.conf.template" \
                    > /etc/systemd/system/grafana-server.service.d/smtp.conf
                chmod 640 /etc/systemd/system/grafana-server.service.d/smtp.conf
                log_info "Grafana SMTP configured: $SMTP_HOST:$SMTP_PORT_VAL as $FROM_ADDR"

                # Need to reload systemd and restart Grafana for environment changes
                systemctl daemon-reload
                GRAFANA_NEEDS_RESTART=true
            fi

            # Set ownership to grafana user
            if id "grafana" &>/dev/null; then
                chown -R grafana:grafana /etc/grafana/provisioning/alerting
                log_info "Grafana alerting configuration installed successfully"
                GRAFANA_NEEDS_RESTART=true
            else
                log_warn "grafana user not found, skipping ownership change"
            fi
        else
            log_warn "Environment file $ENV_FILE not found, skipping alerting configuration"
        fi
    else
        log_info "No alerting configuration found in deployment, skipping"
    fi
fi

# Install Grafana dashboard JSON files
DASHBOARD_COUNT=0
if compgen -G "$DEPLOY_DIR/grafana-dashboard-*.json" > /dev/null; then
    log_info "Installing Grafana dashboard files..."
    mkdir -p /etc/grafana/dashboards

    # Remove old SOAR Grafana dashboards that aren't in the deployment
    if [ -d "/etc/grafana/dashboards" ]; then
        log_info "Cleaning up old Grafana dashboards..."
        for old_dashboard in /etc/grafana/dashboards/grafana-dashboard-*.json; do
            if [ -f "$old_dashboard" ]; then
                dashboard_basename=$(basename "$old_dashboard")
                if [ ! -f "$DEPLOY_DIR/$dashboard_basename" ]; then
                    log_info "Removing obsolete Grafana dashboard: $dashboard_basename"
                    rm -f "$old_dashboard"
                fi
            fi
        done
    fi

    cp "$DEPLOY_DIR"/grafana-dashboard-*.json /etc/grafana/dashboards/
    chmod 644 /etc/grafana/dashboards/*.json

    # Set ownership to grafana user if it exists
    if id "grafana" &>/dev/null; then
        chown -R grafana:grafana /etc/grafana/dashboards
        DASHBOARD_COUNT=$(ls -1 "$DEPLOY_DIR"/grafana-dashboard-*.json | wc -l)
        log_info "Installed $DASHBOARD_COUNT Grafana dashboard(s)"
        GRAFANA_NEEDS_RESTART=true
    else
        log_warn "grafana user not found, skipping ownership change"
    fi
else
    log_warn "No Grafana dashboard files found in deployment, skipping dashboard installation"
fi

# Restart Grafana if dashboards or provisioning was updated
if [ "$GRAFANA_NEEDS_RESTART" = true ]; then
    if systemctl is-active --quiet grafana-server; then
        # Clean up datasources that will be provisioned to prevent UID conflicts
        log_info "Cleaning up datasources that will be re-provisioned..."
        if [ -f /var/lib/grafana/grafana.db ]; then
            # Extract UIDs and NAMEs from provisioning files
            declare -A DATASOURCES_TO_REMOVE  # associative array: uid -> name
            if [ -d /etc/grafana/provisioning/datasources ]; then
                for datasource_file in /etc/grafana/provisioning/datasources/*.yml /etc/grafana/provisioning/datasources/*.yaml; do
                    if [ -f "$datasource_file" ]; then
                        # Parse YAML to extract datasource entries (simple approach for our known structure)
                        # Extract UID and NAME pairs from the provisioning file
                        current_uid=""
                        current_name=""

                        while IFS= read -r line; do
                            # Check for UID line
                            if echo "$line" | grep -qE '^\s*uid:\s*\S+'; then
                                current_uid=$(echo "$line" | awk '{print $2}' | tr -d '"' | tr -d "'")
                            fi
                            # Check for NAME line (with optional list item dash)
                            if echo "$line" | grep -qE '^\s*-?\s*name:\s*.+'; then
                                # Extract name, handling quoted and unquoted values, and list item dash
                                current_name=$(echo "$line" | sed 's/^\s*-\?\s*name:\s*//' | sed 's/^["'\'']//' | sed 's/["'\'']$//')
                            fi
                            # When we have both uid and name, store them
                            if [ -n "$current_uid" ] && [ -n "$current_name" ]; then
                                DATASOURCES_TO_REMOVE["$current_uid"]="$current_name"
                                log_info "  Found provisioned datasource: uid='$current_uid', name='$current_name'"
                                current_uid=""
                                current_name=""
                            fi
                        done < "$datasource_file"
                    fi
                done
            fi

            # Remove datasources with matching UIDs or NAMEs from database
            if [ ${#DATASOURCES_TO_REMOVE[@]} -gt 0 ]; then
                for uid in "${!DATASOURCES_TO_REMOVE[@]}"; do
                    name="${DATASOURCES_TO_REMOVE[$uid]}"

                    # Delete by UID (exact match)
                    EXISTING_DS=$(sudo sqlite3 /var/lib/grafana/grafana.db "SELECT name FROM data_source WHERE uid='$uid';" 2>/dev/null || true)
                    if [ -n "$EXISTING_DS" ]; then
                        log_info "  Removing datasource with uid='$uid' (name='$EXISTING_DS')..."
                        sudo sqlite3 /var/lib/grafana/grafana.db "DELETE FROM data_source WHERE uid='$uid';" 2>/dev/null || log_warn "Failed to delete datasource uid='$uid'"
                    fi

                    # Also delete by NAME (case-insensitive) to catch manually created datasources
                    EXISTING_DS_BY_NAME=$(sudo sqlite3 /var/lib/grafana/grafana.db "SELECT uid, name FROM data_source WHERE LOWER(name)=LOWER('$name');" 2>/dev/null || true)
                    if [ -n "$EXISTING_DS_BY_NAME" ]; then
                        log_info "  Removing datasource(s) with name='$name': $EXISTING_DS_BY_NAME"
                        sudo sqlite3 /var/lib/grafana/grafana.db "DELETE FROM data_source WHERE LOWER(name)=LOWER('$name');" 2>/dev/null || log_warn "Failed to delete datasource name='$name'"
                    fi
                done

                log_info "Datasource cleanup complete - provisioning will recreate them with correct UIDs"
            else
                log_info "  No datasources found in provisioning files"
            fi
        else
            log_warn "Grafana database not found at /var/lib/grafana/grafana.db, skipping datasource cleanup"
        fi

        log_info "Restarting Grafana to load new dashboards..."
        systemctl restart grafana-server || log_warn "Failed to restart Grafana"
        log_info "Grafana restarted successfully"

        # Wait for Grafana to fully start
        sleep 3

        # Validate required datasources exist
        log_info "Validating Grafana datasources..."
        DATASOURCE_VALIDATION_FAILED=false

        # Check for prometheus datasource (required for alerts and dashboards)
        # First check by exact UID match (provisioned as uid='prometheus')
        PROM_UID_EXISTS=$(sudo sqlite3 /var/lib/grafana/grafana.db "SELECT uid FROM data_source WHERE uid='prometheus';" 2>/dev/null || true)

        if [ -n "$PROM_UID_EXISTS" ]; then
            log_info "Datasource 'prometheus': ${GREEN}FOUND (by UID)${NC}"
        else
            # Check if datasource exists with different UID but matching name (case-insensitive)
            PROM_NAME_EXISTS=$(sudo sqlite3 /var/lib/grafana/grafana.db "SELECT uid, name FROM data_source WHERE LOWER(name)='prometheus';" 2>/dev/null || true)

            if [ -n "$PROM_NAME_EXISTS" ]; then
                log_warn "Prometheus datasource found by name but with different UID: $PROM_NAME_EXISTS"
                log_warn "Expected uid='prometheus' per provisioning config at infrastructure/grafana-provisioning/datasources/prometheus.yaml"
                log_warn "This may indicate Grafana provisioning didn't run or was overridden manually"
                log_warn "Alerts and dashboards reference uid='prometheus' and may not work correctly"
                DATASOURCE_VALIDATION_FAILED=true
            else
                log_warn "Prometheus datasource not found in Grafana database"
                log_warn "Expected datasource with uid='prometheus' per provisioning config"
                log_warn "Alerts and dashboards will not work without this datasource"
                log_warn "Grafana should auto-provision this datasource on next restart if config is present"
                DATASOURCE_VALIDATION_FAILED=true
            fi
        fi

        # Check for PostgreSQL datasource (required for flight analysis dashboard)
        # Check by both UID and name for flexibility
        if [ "$ENVIRONMENT" = "staging" ]; then
            POSTGRES_DATASOURCE_UID="soar-postgres-staging"
            POSTGRES_DATASOURCE_NAME="soar-postgres-staging"
        else
            POSTGRES_DATASOURCE_UID="soar-postgres"
            POSTGRES_DATASOURCE_NAME="soar-postgres"
        fi

        # Check if datasource exists by UID or name
        POSTGRES_UID_EXISTS=$(sudo sqlite3 /var/lib/grafana/grafana.db "SELECT uid FROM data_source WHERE uid='$POSTGRES_DATASOURCE_UID';" 2>/dev/null || true)
        POSTGRES_NAME_EXISTS=$(sudo sqlite3 /var/lib/grafana/grafana.db "SELECT uid FROM data_source WHERE name='$POSTGRES_DATASOURCE_NAME' OR name='soar-postgres';" 2>/dev/null || true)

        if [ -n "$POSTGRES_UID_EXISTS" ]; then
            log_info "Datasource '$POSTGRES_DATASOURCE_UID': ${GREEN}FOUND (by UID)${NC}"
        elif [ -n "$POSTGRES_NAME_EXISTS" ]; then
            log_info "Datasource 'soar-postgres': ${GREEN}FOUND (by name)${NC}"
            log_warn "PostgreSQL datasource found but UID doesn't match expected '$POSTGRES_DATASOURCE_UID'"
            log_warn "Flight analysis dashboard queries may need to reference the correct datasource"
        else
            log_warn "Datasource 'soar-postgres' or '$POSTGRES_DATASOURCE_UID' not found in Grafana"
            log_warn "Flight analysis dashboard will not work without this datasource"
            log_warn "To fix: Add PostgreSQL datasource in Grafana UI"
        fi

        if [ "$DATASOURCE_VALIDATION_FAILED" = true ]; then
            log_warn "${YELLOW}Datasource validation warnings detected - Grafana alerts may not work${NC}"
            log_warn "Review datasource configuration in Grafana UI or check provisioning logs"
            log_warn "Continuing with deployment..."
        fi
    else
        log_warn "Grafana is not running, skipping restart and datasource validation"
    fi
fi

# Install observability stack configuration files (Tempo, Loki, Pyroscope, Alloy)
log_info "Installing observability stack configuration..."

# Install Tempo configuration
if [ -f "$DEPLOY_DIR/tempo-config.yml" ]; then
    log_info "Installing Tempo configuration..."
    sudo cp "$DEPLOY_DIR/tempo-config.yml" /etc/tempo/config.yml
    sudo chown tempo:nogroup /etc/tempo/config.yml
    sudo chmod 644 /etc/tempo/config.yml
    log_info "Tempo configuration installed"

    # Restart Tempo if running to pick up new config
    if systemctl is-active --quiet tempo; then
        log_info "Restarting Tempo to apply new configuration..."
        systemctl restart tempo || log_warn "Failed to restart Tempo"
    fi
else
    log_warn "Tempo configuration file not found in deployment directory"
fi

# Install Loki configuration
if [ -f "$DEPLOY_DIR/loki-config.yml" ]; then
    log_info "Installing Loki configuration..."
    sudo cp "$DEPLOY_DIR/loki-config.yml" /etc/loki/config.yml
    sudo chown loki:nogroup /etc/loki/config.yml 2>/dev/null || sudo chown nobody:nogroup /etc/loki/config.yml
    sudo chmod 644 /etc/loki/config.yml
    log_info "Loki configuration installed"

    # Restart Loki if running to pick up new config
    if systemctl is-active --quiet loki; then
        log_info "Restarting Loki to apply new configuration..."
        systemctl restart loki || log_warn "Failed to restart Loki"
    fi
else
    log_warn "Loki configuration file (loki-config.yml) not found in deployment directory"
fi

# Install Pyroscope configuration
if [ -f "$DEPLOY_DIR/pyroscope-config.yml" ]; then
    log_info "Installing Pyroscope configuration..."
    sudo cp "$DEPLOY_DIR/pyroscope-config.yml" /etc/pyroscope/config.yml
    sudo chown pyroscope:pyroscope /etc/pyroscope/config.yml
    sudo chmod 644 /etc/pyroscope/config.yml
    log_info "Pyroscope configuration installed"

    # Restart Pyroscope if running to pick up new config
    if systemctl is-active --quiet pyroscope; then
        log_info "Restarting Pyroscope to apply new configuration..."
        systemctl restart pyroscope || log_warn "Failed to restart Pyroscope"
    fi
else
    log_warn "Pyroscope configuration file not found in deployment directory"
fi

# Install Alloy configuration (template with git commit for source code linking)
if [ -f "$DEPLOY_DIR/alloy-config.alloy.template" ]; then
    log_info "Processing Alloy configuration template..."

    # Get git commit from VERSION file (set during CI)
    if [ -f "$DEPLOY_DIR/VERSION" ]; then
        GIT_COMMIT=$(cat "$DEPLOY_DIR/VERSION" | tr -d '\n' | tr -d '\r')
        log_info "Git commit for profiling source links: $GIT_COMMIT"
    else
        GIT_COMMIT="unknown"
        log_warn "VERSION file not found, profiling source links will not work"
    fi

    # Process template and install
    sed -e "s|{{GIT_COMMIT}}|$GIT_COMMIT|g" \
        "$DEPLOY_DIR/alloy-config.alloy.template" \
        > /etc/alloy/config.alloy
    sudo chown alloy:alloy /etc/alloy/config.alloy
    sudo chmod 644 /etc/alloy/config.alloy
    log_info "Alloy configuration installed with git commit: $GIT_COMMIT"

    # Restart Alloy if running to pick up new config
    if systemctl is-active --quiet alloy; then
        log_info "Restarting Alloy to apply new configuration..."
        systemctl restart alloy || log_warn "Failed to restart Alloy"
    fi
elif [ -f "$DEPLOY_DIR/alloy-config.alloy" ]; then
    # Fallback to non-template version if present
    log_info "Installing Alloy configuration (non-template)..."
    sudo cp "$DEPLOY_DIR/alloy-config.alloy" /etc/alloy/config.alloy
    sudo chown alloy:alloy /etc/alloy/config.alloy
    sudo chmod 644 /etc/alloy/config.alloy
    log_info "Alloy configuration installed"

    # Restart Alloy if running to pick up new config
    if systemctl is-active --quiet alloy; then
        log_info "Restarting Alloy to apply new configuration..."
        systemctl restart alloy || log_warn "Failed to restart Alloy"
    fi
else
    log_warn "Alloy configuration file not found in deployment directory"
fi

# Install PgBouncer configuration
if [ -f "$DEPLOY_DIR/pgbouncer.ini" ]; then
    # Check if config has changed before installing
    if ! diff -q "$DEPLOY_DIR/pgbouncer.ini" /etc/pgbouncer/pgbouncer.ini >/dev/null 2>&1; then
        log_info "Installing PgBouncer configuration..."
        sudo cp "$DEPLOY_DIR/pgbouncer.ini" /etc/pgbouncer/pgbouncer.ini
        sudo chown postgres:postgres /etc/pgbouncer/pgbouncer.ini
        sudo chmod 640 /etc/pgbouncer/pgbouncer.ini
        log_info "PgBouncer configuration installed"

        # Reload PgBouncer if running to pick up new config (graceful, no connection drop)
        if systemctl is-active --quiet pgbouncer; then
            log_info "Reloading PgBouncer to apply new configuration..."
            if ! systemctl reload pgbouncer; then
                log_warn "Failed to reload PgBouncer, attempting restart..."
                systemctl restart pgbouncer || log_warn "Failed to restart PgBouncer"
            fi
        fi
    else
        log_info "PgBouncer configuration unchanged, skipping"
    fi
else
    log_warn "PgBouncer configuration file not found in deployment directory"
fi

# Reload systemd daemon
log_info "Reloading systemd daemon..."
systemctl daemon-reload

# Remove obsolete partman systemd files (pg_partman replaced by TimescaleDB)
log_info "Removing obsolete partman systemd files..."
PARTMAN_FILES_REMOVED=0
for partman_file in "partman-maintenance${SERVICE_SUFFIX}.service" "partman-maintenance${SERVICE_SUFFIX}.timer"; do
    if [ -f "/etc/systemd/system/$partman_file" ]; then
        log_info "Stopping and disabling $partman_file..."
        systemctl stop "$partman_file" 2>/dev/null || true
        systemctl disable "$partman_file" 2>/dev/null || true
        log_info "Removing $partman_file..."
        rm -f "/etc/systemd/system/$partman_file"
        ((PARTMAN_FILES_REMOVED++)) || true
    fi
done

if [ $PARTMAN_FILES_REMOVED -gt 0 ]; then
    log_info "Removed $PARTMAN_FILES_REMOVED partman systemd file(s)"
    systemctl daemon-reload
else
    log_info "No partman systemd files found (already removed or not present)"
fi

# Enable target file
if [ -f "/etc/systemd/system/$TARGET_FILE" ]; then
    log_info "Enabling $TARGET_FILE..."
    systemctl enable "$TARGET_FILE" || log_warn "Failed to enable $TARGET_FILE"
fi

# Enable and start services
log_info "Enabling and starting SOAR services..."

# Start active services
for service in "${ACTIVE_SERVICES[@]}"; do
    if [ -f "/etc/systemd/system/$service" ]; then
        log_info "Enabling $service..."
        systemctl enable "$service" || log_warn "Failed to enable $service"

        log_info "Starting $service..."
        systemctl start "$service" || log_warn "Failed to start $service"
    fi
done

# Restart manual services that were running before deployment
if [ ${#MANUAL_SERVICES_TO_RESTART[@]} -gt 0 ]; then
    log_info "Restarting manual services that were previously running..."
    for service in "${MANUAL_SERVICES_TO_RESTART[@]}"; do
        if [ -f "/etc/systemd/system/$service" ]; then
            log_info "Enabling $service..."
            systemctl enable "$service" || log_warn "Failed to enable $service"

            log_info "Starting $service..."
            systemctl start "$service" || log_warn "Failed to start $service"
        fi
    done
else
    log_info "No manual services were running before deployment, none will be restarted"
    log_info "Manual services (${MANUAL_SERVICES[*]}) are installed but must be started manually if needed"
fi

# Timer-invoked services are installed but not enabled or started
log_info "Timer-invoked services (${TIMER_SERVICES[*]}) will only be invoked by their timers"

# Enable and start timers only if they're not already running
# Timers pick up configuration changes via daemon-reload without restart
log_info "Ensuring SOAR timers are enabled and active..."
for timer in "${TIMERS[@]}"; do
    if [ -f "/etc/systemd/system/$timer" ]; then
        # Enable timer for automatic start on boot
        systemctl enable "$timer" || log_warn "Failed to enable $timer"

        # Start timer only if it's not already active (never restart to avoid triggering Persistent logic)
        if ! systemctl is-active --quiet "$timer"; then
            log_info "Starting $timer (was not active)..."
            systemctl start "$timer" || log_warn "Failed to start $timer"
        else
            log_info "$timer is already active and will continue on schedule"
        fi
    fi
done

# Wait for services to start
log_info "Waiting for services to initialize..."
sleep 5

# Check service status
log_info "Checking service status..."
ALL_HEALTHY=true

# Check active services (always started)
for service in "${ACTIVE_SERVICES[@]}"; do
    if systemctl is-active --quiet "$service"; then
        log_info "$service: ${GREEN}ACTIVE${NC}"
    else
        log_error "$service: ${RED}FAILED${NC}"
        ALL_HEALTHY=false

        # Show recent logs for failed service
        log_info "Recent logs for $service:"
        journalctl -u "$service" --no-pager --lines=10 || true
    fi
done

# Check manual services that were restarted
if [ ${#MANUAL_SERVICES_TO_RESTART[@]} -gt 0 ]; then
    log_info "Checking restarted manual services..."
    for service in "${MANUAL_SERVICES_TO_RESTART[@]}"; do
        if systemctl is-active --quiet "$service"; then
            log_info "$service: ${GREEN}ACTIVE${NC}"
        else
            log_error "$service: ${RED}FAILED${NC}"
            ALL_HEALTHY=false

            # Show recent logs for failed service
            log_info "Recent logs for $service:"
            journalctl -u "$service" --no-pager --lines=10 || true
        fi
    done
fi

# Check timer-invoked services are not running (they should be stopped)
for service in "${TIMER_SERVICES[@]}"; do
    if systemctl is-active --quiet "$service"; then
        log_warn "$service: ${YELLOW}RUNNING${NC} (should only run via timer)"
    else
        log_info "$service: ${GREEN}STOPPED${NC} (timer-invoked only)"
    fi
done

for timer in "${TIMERS[@]}"; do
    if systemctl is-enabled --quiet "$timer" 2>/dev/null; then
        if systemctl is-active --quiet "$timer"; then
            log_info "$timer: ${GREEN}ENABLED and ACTIVE${NC}"
        else
            log_info "$timer: ${GREEN}ENABLED${NC} (will activate on next boot or scheduled time)"
        fi
    else
        log_warn "$timer: ${YELLOW}NOT ENABLED${NC}"
    fi
done

# Show recent logs from all services
log_info "Recent logs from soar-run${SERVICE_SUFFIX}:"
journalctl -u "soar-run${SERVICE_SUFFIX}.service" --no-pager --lines=5 || true

log_info "Recent logs from soar-web${SERVICE_SUFFIX}:"
journalctl -u "soar-web${SERVICE_SUFFIX}.service" --no-pager --lines=5 || true

# Clean up old backups (keep last 5)
log_info "Cleaning up old backups in $BACKUP_DIR (keeping last 5)..."
ls -t "$BACKUP_DIR"/soar.backup.* 2>/dev/null | tail -n +6 | xargs rm -f || true

# Clean up old deployment directories (keep last 3)
log_info "Cleaning up old deployment directories (keeping last 3)..."
ls -td /tmp/soar/deploy/* 2>/dev/null | tail -n +4 | xargs rm -rf || true

# Clean up old deployment logs and status files (keep last 10)
log_info "Cleaning up old deployment logs and status files (keeping last 10)..."
ls -t "$DEPLOY_LOG_DIR"/*.log 2>/dev/null | tail -n +11 | xargs rm -f || true
ls -t "$DEPLOY_STATUS_DIR"/*.status 2>/dev/null | tail -n +11 | xargs rm -f || true

if [ "$ALL_HEALTHY" = true ]; then
    log_info "${GREEN}Deployment completed successfully!${NC}"
    exit 0
else
    log_error "${RED}Deployment completed with errors. Some services failed to start.${NC}"
    exit 1
fi

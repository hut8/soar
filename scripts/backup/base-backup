#!/bin/bash
#
# base-backup - Create PostgreSQL database backup and upload to Wasabi S3
#
# This script creates a logical backup of a single PostgreSQL database using
# pg_dump, uploads to Wasabi S3 via rclone, and manages retention.
#
# Usage:
#   base-backup [--no-cleanup]
#
# Options:
#   --no-cleanup    Skip cleanup of old backups (useful for testing)
#
# Exit codes:
#   0 - Success
#   1 - Failure
#
# Configuration:
#   Source credentials from /etc/soar/backup-env
#   Requires rclone configured with Wasabi remote
#
# Scheduling:
#   Run via systemd timer: soar-backup-base.timer (daily)

set -euo pipefail

# Load configuration
if [[ ! -f /etc/soar/backup-env ]]; then
    echo "ERROR: /etc/soar/backup-env not found" >&2
    exit 1
fi

source /etc/soar/backup-env

# Parse arguments
CLEANUP=true
while [[ $# -gt 0 ]]; do
    case "$1" in
        --no-cleanup)
            CLEANUP=false
            shift
            ;;
        *)
            echo "Unknown option: $1" >&2
            exit 1
            ;;
    esac
done

# Rclone configuration
RCLONE_REMOTE="${BACKUP_RCLONE_REMOTE:-wasabi}"
RCLONE_BUCKET="${BACKUP_RCLONE_BUCKET:-soar-backup-prod}"
RCLONE_PATH="${BACKUP_RCLONE_PATH:-}"  # Optional prefix within bucket

# Validate configuration
if [[ -z "${BACKUP_RCLONE_REMOTE:-}" ]]; then
    echo "ERROR: BACKUP_RCLONE_REMOTE not set in /etc/soar/backup-env" >&2
    exit 1
fi

if [[ -z "${BACKUP_RCLONE_BUCKET:-}" ]]; then
    echo "ERROR: BACKUP_RCLONE_BUCKET not set in /etc/soar/backup-env" >&2
    exit 1
fi

# Check rclone is available
if ! command -v rclone >/dev/null 2>&1; then
    echo "ERROR: rclone not found in PATH" >&2
    exit 1
fi

# Verify rclone remote is configured
if ! rclone listremotes | grep -q "^${RCLONE_REMOTE}:$"; then
    echo "ERROR: rclone remote '${RCLONE_REMOTE}' not configured" >&2
    echo "Run: rclone config" >&2
    exit 1
fi

# Directories
TEMP_DIR="${BACKUP_TEMP_DIR:-/storage/soar/backups/base}"
mkdir -p "$TEMP_DIR"

# Database connection
PGHOST="${PGHOST:-localhost}"
PGPORT="${PGPORT:-5432}"
PGDATABASE="${PGDATABASE:-soar}"
PGUSER="${PGUSER:-postgres}"

# Backup configuration
COMPRESSION="${BACKUP_COMPRESSION:-gzip}"
PARALLEL_JOBS="${BACKUP_PARALLEL_JOBS:-4}"
RETENTION_DAYS="${BACKUP_RETENTION_DAYS:-30}"

# Backup identifier (date)
BACKUP_DATE=$(date -u +"%Y-%m-%d")
BACKUP_TIMESTAMP=$(date -u +"%Y-%m-%d_%H-%M-%S")
BACKUP_NAME="backup-${BACKUP_TIMESTAMP}"
BACKUP_LOCAL_DIR="$TEMP_DIR/$BACKUP_NAME"

# Construct rclone destination path
if [[ -n "$RCLONE_PATH" ]]; then
    BACKUP_REMOTE_PATH="${RCLONE_REMOTE}:${RCLONE_BUCKET}/${RCLONE_PATH}/base/${BACKUP_DATE}"
else
    BACKUP_REMOTE_PATH="${RCLONE_REMOTE}:${RCLONE_BUCKET}/base/${BACKUP_DATE}"
fi

# Metrics and notifications
NOTIFY_EMAIL="${BACKUP_NOTIFY_EMAIL:-}"
NOTIFY_SLACK="${BACKUP_NOTIFY_SLACK_WEBHOOK:-}"
SENTRY_DSN="${SENTRY_DSN:-}"

# Function to log messages
log() {
    local level="$1"
    shift
    echo "[$(date -u +"%Y-%m-%d %H:%M:%S UTC")] [$level] Base Backup: $*"
}

# Function to send notifications
notify() {
    local status="$1"
    local message="$2"

    log INFO "Notification: $status - $message"

    # Email notification
    if [[ -n "$NOTIFY_EMAIL" ]] && command -v mail >/dev/null 2>&1; then
        echo "$message" | mail -s "SOAR Backup: $status" "$NOTIFY_EMAIL"
    fi

    # Slack notification
    if [[ -n "$NOTIFY_SLACK" ]] && command -v curl >/dev/null 2>&1; then
        curl -X POST "$NOTIFY_SLACK" \
            -H 'Content-Type: application/json' \
            -d "{\"text\":\"SOAR Backup: $status\n$message\"}" \
            >/dev/null 2>&1 || true
    fi

    # Sentry notification
    if [[ -n "$SENTRY_DSN" ]] && command -v curl >/dev/null 2>&1; then
        # Extract project ID and key from DSN
        # DSN format: https://<key>@<organization>.ingest.sentry.io/<project_id>
        local sentry_key=$(echo "$SENTRY_DSN" | sed -n 's|https://\([^@]*\)@.*|\1|p')
        local sentry_host=$(echo "$SENTRY_DSN" | sed -n 's|https://[^@]*@\([^/]*\).*|\1|p')
        local sentry_project=$(echo "$SENTRY_DSN" | sed -n 's|.*/\([0-9]*\)$|\1|p')

        if [[ -n "$sentry_key" ]] && [[ -n "$sentry_host" ]] && [[ -n "$sentry_project" ]]; then
            local event_level="info"
            [[ "$status" == "FAILED" ]] && event_level="error"
            [[ "$status" == "SUCCESS" ]] && event_level="info"

            local event_payload=$(cat <<EOF
{
  "event_id": "$(uuidgen | tr -d '-' | tr '[:upper:]' '[:lower:]')",
  "timestamp": "$(date -u +"%Y-%m-%dT%H:%M:%S")",
  "platform": "other",
  "level": "$event_level",
  "logger": "soar-backup",
  "message": "SOAR Backup: $status",
  "extra": {
    "backup_name": "${BACKUP_NAME:-unknown}",
    "backup_date": "${BACKUP_DATE:-unknown}",
    "hostname": "$(hostname)",
    "message": "$message",
    "script": "base-backup"
  },
  "tags": {
    "backup_type": "base",
    "status": "$status",
    "environment": "production"
  }
}
EOF
)

            curl -X POST "https://${sentry_host}/api/${sentry_project}/store/" \
                -H "X-Sentry-Auth: Sentry sentry_version=7, sentry_key=${sentry_key}, sentry_client=soar-backup/1.0" \
                -H 'Content-Type: application/json' \
                -d "$event_payload" \
                >/dev/null 2>&1 || true
        fi
    fi
}

# Function to cleanup on exit
cleanup() {
    local exit_code=$?
    if [[ $exit_code -ne 0 ]]; then
        log ERROR "Backup failed with exit code $exit_code"
        notify "FAILED" "Base backup failed on $(hostname) at $(date). Check logs with: journalctl -u soar-backup-base"
    fi

    # Remove temporary files
    if [[ -d "$BACKUP_LOCAL_DIR" ]]; then
        log INFO "Cleaning up temporary directory: $BACKUP_LOCAL_DIR"
        rm -rf "$BACKUP_LOCAL_DIR"
    fi
}

trap cleanup EXIT

# Main backup function
main() {
    local start_time=$(date +%s)

    log INFO "=========================================="
    log INFO "Starting database backup: $BACKUP_NAME"
    log INFO "=========================================="
    log INFO "Database: ${PGUSER}@${PGHOST}:${PGPORT}/${PGDATABASE}"
    log INFO "Destination: ${BACKUP_REMOTE_PATH}"
    log INFO "Method: pg_dump (custom format with zstd compression)"
    log INFO "Parallel jobs: $PARALLEL_JOBS"

    # Check database connectivity
    log INFO "Checking database connectivity..."
    if ! psql -h "$PGHOST" -p "$PGPORT" -U "$PGUSER" -d "$PGDATABASE" -c "SELECT 1" >/dev/null 2>&1; then
        log ERROR "Cannot connect to database"
        exit 1
    fi

    # Check available disk space
    local available_space=$(df "$TEMP_DIR" | tail -1 | awk '{print $4}')
    log INFO "Available disk space in $TEMP_DIR: $(numfmt --to=iec-i --suffix=B $((available_space * 1024)))"

    # Get current database size for reference
    local db_size=$(psql -h "$PGHOST" -p "$PGPORT" -U "$PGUSER" -d "$PGDATABASE" -t -c \
        "SELECT pg_database_size('$PGDATABASE')" | tr -d ' ')
    log INFO "Current database size: $(numfmt --to=iec-i --suffix=B "$db_size")"

    # Create database backup using pg_dump
    log INFO "Creating database backup with pg_dump..."
    mkdir -p "$BACKUP_LOCAL_DIR"

    # Use custom format (-Fc) which is compressed and supports parallel restore
    # Use parallel jobs for faster dump of large databases
    local dump_file="$BACKUP_LOCAL_DIR/database.dump"

    log INFO "Using $PARALLEL_JOBS parallel jobs"

    if ! pg_dump \
        -h "$PGHOST" \
        -p "$PGPORT" \
        -U "$PGUSER" \
        -d "$PGDATABASE" \
        -Fc \
        -Z zstd:3 \
        -j "$PARALLEL_JOBS" \
        -f "$dump_file" \
        -v; then
        log ERROR "pg_dump failed"
        exit 1
    fi

    log INFO "Database backup completed successfully"

    # Get backup size
    local backup_size=$(du -sb "$dump_file" | cut -f1)
    log INFO "Backup size: $(numfmt --to=iec-i --suffix=B "$backup_size")"

    # Upload to Wasabi S3 using rclone
    log INFO "Uploading backup to Wasabi S3..."

    # Use rclone copy with multiple transfers (no progress to keep logs clean)
    if ! rclone copy \
        --transfers "$PARALLEL_JOBS" \
        --s3-chunk-size 64M \
        --s3-upload-concurrency "$PARALLEL_JOBS" \
        "$BACKUP_LOCAL_DIR/" \
        "${BACKUP_REMOTE_PATH}/"; then
        log ERROR "Failed to upload backup to Wasabi S3"
        exit 1
    fi

    log INFO "Upload completed successfully"

    # Verify uploaded files
    log INFO "Verifying uploaded files..."
    local uploaded_count=$(rclone ls "${BACKUP_REMOTE_PATH}/" 2>/dev/null | wc -l)

    if [[ $uploaded_count -eq 0 ]]; then
        log ERROR "Verification failed: no files found in Wasabi S3"
        exit 1
    fi

    log INFO "Verification passed: found $uploaded_count files in Wasabi S3"

    # Create metadata file
    log INFO "Creating backup metadata..."
    cat > "$TEMP_DIR/backup-metadata.json" <<EOF
{
  "backup_name": "$BACKUP_NAME",
  "backup_date": "$BACKUP_DATE",
  "backup_timestamp": "$BACKUP_TIMESTAMP",
  "backup_type": "pg_dump",
  "backup_format": "custom",
  "database": "$PGDATABASE",
  "database_size_bytes": $db_size,
  "backup_size_bytes": $backup_size,
  "compression": "zstd",
  "parallel_jobs": $PARALLEL_JOBS,
  "hostname": "$(hostname)",
  "postgresql_version": "$(psql -h "$PGHOST" -p "$PGPORT" -U "$PGUSER" -d "$PGDATABASE" -t -c "SHOW server_version" | tr -d ' ')",
  "completed_at": "$(date -u +"%Y-%m-%d %H:%M:%S UTC")"
}
EOF

    rclone copyto \
        "$TEMP_DIR/backup-metadata.json" \
        "${BACKUP_REMOTE_PATH}/backup-metadata.json"

    # Cleanup old backups if enabled
    if [[ "$CLEANUP" == "true" ]]; then
        # Number of most recent backups to keep (default: 5)
        local keep_count=${BASE_BACKUP_KEEP_COUNT:-5}
        log INFO "Cleaning up old backups (keeping most recent: $keep_count)..."

        # Construct base path for listing backups
        local base_path
        if [[ -n "$RCLONE_PATH" ]]; then
            base_path="${RCLONE_REMOTE}:${RCLONE_BUCKET}/${RCLONE_PATH}/base"
        else
            base_path="${RCLONE_REMOTE}:${RCLONE_BUCKET}/base"
        fi

        # List all base backups (directories starting with 20* for dates like 2025-01-*)
        # rclone lsd lists directories only
        local all_backups=$(rclone lsd "$base_path" 2>/dev/null | \
            awk '{print $5}' | \
            grep '^20[0-9][0-9]-[0-9][0-9]-[0-9][0-9]$' | \
            sort -r || echo "")

        if [[ -z "$all_backups" ]]; then
            log WARN "No backups found for cleanup"
        else
            local backup_count=$(echo "$all_backups" | wc -l)
            log INFO "Found $backup_count total backups"

            if [[ $backup_count -le $keep_count ]]; then
                log INFO "Only $backup_count backups exist, keeping all (configured to keep: $keep_count)"
            else
                # Delete old backups beyond the keep count
                log INFO "Will keep $keep_count most recent backups, deleting $((backup_count - keep_count)) old backups"

                local skip_count=0
                local deleted_count=0

                for backup_date in $all_backups; do
                    if [[ $skip_count -lt $keep_count ]]; then
                        # Keep this backup
                        ((skip_count++))
                        log INFO "Keeping backup: $backup_date ($skip_count of $keep_count)"
                    else
                        # Delete this backup
                        log INFO "Deleting old backup: $backup_date"

                        if rclone purge "${base_path}/${backup_date}"; then
                            ((deleted_count++))
                            log INFO "Successfully deleted backup: $backup_date"
                        else
                            log ERROR "Failed to delete backup: $backup_date"
                        fi
                    fi
                done

                log INFO "Cleanup completed: deleted $deleted_count old backups, kept $keep_count recent backups"
            fi
        fi
    fi

    # Calculate duration
    local end_time=$(date +%s)
    local duration=$((end_time - start_time))
    local duration_formatted=$(printf '%02d:%02d:%02d' $((duration/3600)) $((duration%3600/60)) $((duration%60)))

    log INFO "=========================================="
    log INFO "Backup completed successfully!"
    log INFO "Duration: $duration_formatted"
    log INFO "Backup location: ${BACKUP_REMOTE_PATH}"
    log INFO "=========================================="

    # Send success notification
    notify "SUCCESS" "Database backup completed successfully on $(hostname)
Backup: $BACKUP_NAME
Database: $PGDATABASE
Duration: $duration_formatted
Database size: $(numfmt --to=iec-i --suffix=B "$db_size")
Backup size: $(numfmt --to=iec-i --suffix=B "$backup_size")
Location: ${BACKUP_REMOTE_PATH}"

    # Update metrics (if metrics endpoint is available)
    if [[ -n "${METRICS_PUSHGATEWAY:-}" ]] && command -v curl >/dev/null 2>&1; then
        cat <<EOF | curl --data-binary @- "${METRICS_PUSHGATEWAY}/metrics/job/soar_backup" >/dev/null 2>&1 || true
# TYPE backup_base_duration_seconds gauge
backup_base_duration_seconds $duration
# TYPE backup_base_size_bytes gauge
backup_base_size_bytes $backup_size
# TYPE backup_base_last_success_timestamp gauge
backup_base_last_success_timestamp $(date +%s)
EOF
    fi

    return 0
}

# Run main function
main

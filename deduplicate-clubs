#!/usr/bin/env python3
"""
Club de-duper for PostgreSQL 'soar'.

Default is DRY-RUN (no DB changes). Pass --apply to perform updates/deletes.

Merges:
  1) Exact-normalized duplicates (very safe)
  2) Optional fuzzy clusters (token-set similarity >= threshold)

For each merge group:
  - Choose a canonical "keeper" club (configurable).
  - Repoint foreign keys in:
      * users.club_id
      * aircraft_registrations.club_id
  - Delete the duplicate clubs.

Requires:
  pip install psycopg[binary] rapidfuzz

Usage examples:
  python merge_clubs.py                       # dry-run, exact + fuzzy (default)
  python merge_clubs.py --no-fuzzy           # dry-run, exact only
  python merge_clubs.py --threshold 92       # stricter fuzzy
  python merge_clubs.py --canonical longest  # choose "longest" name
  python merge_clubs.py --apply              # DO IT (in one transaction)
"""

import argparse
import re
import sys
from collections import defaultdict

# ---- DB config ----
DSN = "dbname=soar"
CLUBS_TABLE = "clubs"
CLUBS_ID = "id"
CLUBS_NAME = "name"

# FKs to retarget (table, fk_column)
FK_TABLES = [
    ("users", "club_id"),
    ("aircraft_registrations", "club_id"),
]

# ---- Normalization knobs ----
NOISE_WORDS = {
    "THE", "INC", "INC.", "LLC", "L.L.C.", "CO", "CO.", "COMPANY", "CORP", "CORP.",
    "CORPORATION", "FOUNDATION", "ASSOCIATION", "ASSN", "ASSOC", "ASSOC.",
    "SOCIETY", "CLUB", "FLYING", "AERO", "AIR", "AERONAUTICAL", "NONPROFIT",
    "NFP", "LTD", "LTD.", "TRUST", "INCORPORATED"
}
REPLACEMENTS = {
    "&": "AND",
    "WSSF": "",
    "W.S.S.F.": "",
}
DROP_PARENS = True

# ---- Fuzzy matching ----
DEFAULT_THRESHOLD = 90  # 0..100 token-set similarity

try:
    from rapidfuzz import fuzz
    def fuzzy_score(a: str, b: str) -> int:
        return fuzz.token_set_ratio(a, b)
except Exception:
    import difflib
    def fuzzy_score(a: str, b: str) -> int:
        return int(round(difflib.SequenceMatcher(None, a, b).ratio() * 100))


# --- helpers ---
_ws_re = re.compile(r"\s+")
_punct_re = re.compile(r"[^\w\s&]+")  # keep & so we can map it -> AND
_paren_re = re.compile(r"\s*\([^)]*\)")

def normalize_name(raw: str) -> str:
    s = raw.upper().strip()
    if DROP_PARENS:
        s = _paren_re.sub("", s)
    s = _punct_re.sub(" ", s)
    for k, v in REPLACEMENTS.items():
        s = s.replace(k, v)
    s = _ws_re.sub(" ", s).strip()
    tokens = [t for t in s.split(" ") if t and t not in NOISE_WORDS]
    tokens = sorted(set(tokens))
    return " ".join(tokens)

def choose_canonical(records, strategy="best"):
    """
    records: list of dicts with keys: id, name, norm
    strategy:
      - "lowest-id": smallest ID
      - "longest": longest original name (after trimming)
      - "best" (default): most-informative normalized (more tokens), then longest original, then lowest id
    """
    def score(r):
        # more tokens in normalized -> more informative
        token_count = len(r["norm"].split()) if r["norm"] else 0
        return (token_count, len(r["name"].strip()))

    if strategy == "longest":
        return max(records, key=lambda r: len(r["name"].strip()))
    else:
        return max(records, key=score)

def build_fuzzy_clusters(norm_to_records, threshold):
    """
    norm_to_records: dict[norm] -> list[record]
    Returns list of clusters, where a cluster is list[records].
    """
    norms = [n for n in norm_to_records.keys() if n]
    if not norms:
        return []

    # token blocking
    token_to_idxs = defaultdict(list)
    for i, n in enumerate(norms):
        for tok in n.split():
            token_to_idxs[tok].append(i)

    neighbors = [set() for _ in norms]
    for idxs in token_to_idxs.values():
        for i in idxs:
            neighbors[i].update(idxs)
    for i in range(len(norms)):
        neighbors[i].discard(i)

    # Union-Find
    parent = list(range(len(norms)))
    rank = [0]*len(norms)

    def find(x):
        while parent[x] != x:
            parent[x] = parent[parent[x]]
            x = parent[x]
        return x

    def union(a, b):
        ra, rb = find(a), find(b)
        if ra == rb:
            return
        if rank[ra] < rank[rb]:
            parent[ra] = rb
        elif rank[rb] < rank[ra]:
            parent[rb] = ra
        else:
            parent[rb] = ra
            rank[ra] += 1

    # link sufficiently similar
    for i, a in enumerate(norms):
        for j in neighbors[i]:
            if j <= i:
                continue
            b = norms[j]
            if fuzzy_score(a, b) >= threshold:
                union(i, j)

    # collect clusters (skip singletons)
    groups = defaultdict(list)
    for i, n in enumerate(norms):
        root = find(i)
        groups[root].append(n)

    clusters = []
    for _, norm_list in groups.items():
        if len(norm_list) <= 1:
            continue
        recs = []
        for n in norm_list:
            recs.extend(norm_to_records[n])
        clusters.append(recs)
    return clusters

def main():
    ap = argparse.ArgumentParser(description="Suggest and optionally apply club merges.")
    ap.add_argument("--apply", action="store_true", help="Apply changes (otherwise dry-run).")
    ap.add_argument("--no-fuzzy", action="store_true", help="Only merge exact-normalized duplicates.")
    ap.add_argument("--threshold", type=int, default=DEFAULT_THRESHOLD, help="Fuzzy match threshold (0..100).")
    ap.add_argument("--canonical", choices=["best", "lowest-id", "longest"], default="best",
                    help="How to pick the canonical club in a group.")
    ap.add_argument("--show-empty-norm", action="store_true", help="Also show items whose normalized form is empty.")
    args = ap.parse_args()

    try:
        import psycopg
    except Exception:
        print("Please install psycopg (v3): pip install psycopg[binary]", file=sys.stderr)
        sys.exit(1)

    # Fetch clubs
    with psycopg.connect(DSN) as conn:
        with conn.cursor() as cur:
            cur.execute(f"""
                SELECT {CLUBS_ID}, {CLUBS_NAME}
                FROM {CLUBS_TABLE}
                WHERE {CLUBS_NAME} IS NOT NULL AND btrim({CLUBS_NAME}) <> ''
            """)
            rows = cur.fetchall()

    if not rows:
        print("No clubs found.")
        return

    # Build records + normalization
    records = []
    by_norm = defaultdict(list)
    for cid, raw in rows:
        norm = normalize_name(raw)
        rec = {"id": cid, "name": raw, "norm": norm}
        records.append(rec)
        by_norm[norm].append(rec)

    # Exact-normalized duplicates (norm groups > 1)
    exact_groups = [recs for n, recs in by_norm.items() if n and len(recs) > 1]
    # Optionally include empty normalized string duplicates (rare, but could exist)
    if args.show_empty_norm:
        exact_groups += [recs for n, recs in by_norm.items() if n == "" and len(recs) > 1]

    # Fuzzy clusters on norms
    fuzzy_groups = []
    if not args.no_fuzzy:
        fuzzy_groups = build_fuzzy_clusters({n: recs for n, recs in by_norm.items() if n}, args.threshold)

    # Deduplicate overlaps: If a record appears in an exact group, don't also propose it in fuzzy group.
    used_ids = set()
    merged_groups = []

    # First, take exact groups (safer)
    for group in exact_groups:
        keep = choose_canonical(group, args.canonical)
        dups = [r for r in group if r["id"] != keep["id"]]
        if dups:
            merged_groups.append((keep, dups, "exact"))
            used_ids.update([r["id"] for r in group])

    # Then, fuzzy groups excluding records already scheduled
    for group in fuzzy_groups:
        group = [r for r in group if r["id"] not in used_ids]
        if len(group) <= 1:
            continue
        keep = choose_canonical(group, args.canonical)
        dups = [r for r in group if r["id"] != keep["id"]]
        if dups:
            merged_groups.append((keep, dups, "fuzzy"))
            used_ids.update([r["id"] for r in group])

    if not merged_groups:
        print("No merge candidates found with current settings.")
        return

    # Report plan
    print("=== MERGE PLAN ===")
    total_dups = 0
    for keep, dups, kind in merged_groups:
        print(f"\n[{kind.upper()}] Keep: [{keep['id']}] {keep['name']!r}  (norm='{keep['norm']}')")
        for d in dups:
            print(f"   Merge -> [{d['id']}] {d['name']!r}  (norm='{d['norm']}')")
        total_dups += len(dups)
    print(f"\nGroups: {len(merged_groups)}, Duplicates to merge: {total_dups}")

    # Apply or dry-run
    if not args.apply:
        print("\nDRY-RUN only. Re-run with --apply to perform updates/deletes.")
        return

    # Apply everything in ONE transaction for atomicity
    with psycopg.connect(DSN) as conn:
        # Optional: enforce a statement timeout
        # with conn.cursor() as cur:
        #     cur.execute("SET LOCAL statement_timeout = '10s'")

        try:
            with conn.transaction():
                with conn.cursor() as cur:
                    for keep, dups, kind in merged_groups:
                        keep_id = keep["id"]
                        dup_ids = [d["id"] for d in dups]

                        # Lock the rows we will touch to avoid races (optional, conservative)
                        cur.execute(
                            f"SELECT {CLUBS_ID} FROM {CLUBS_TABLE} WHERE {CLUBS_ID} = %s FOR UPDATE",
                            (keep_id,)
                        )
                        cur.execute(
                            f"SELECT {CLUBS_ID} FROM {CLUBS_TABLE} WHERE {CLUBS_ID} = ANY(%s) FOR UPDATE",
                            (dup_ids,)
                        )

                        # Repoint FKs
                        for tbl, fkcol in FK_TABLES:
                            cur.execute(
                                f"UPDATE {tbl} SET {fkcol} = %s WHERE {fkcol} = ANY(%s)",
                                (keep_id, dup_ids)
                            )
                            print(f"Updated {tbl}.{fkcol} -> {keep_id} for {len(dup_ids)} duplicate id(s) (ids: {dup_ids})")

                        # Delete the duplicates
                        cur.execute(
                            f"DELETE FROM {CLUBS_TABLE} WHERE {CLUBS_ID} = ANY(%s)",
                            (dup_ids,)
                        )
                        print(f"Deleted from {CLUBS_TABLE}: {dup_ids}")

            print("\nAPPLY complete. All changes committed.")
        except Exception as e:
            print(f"\nERROR: {e}")
            print("Transaction rolled back; no changes were committed.")
            raise  # re-raise to get a traceback if running interactively


if __name__ == "__main__":
    main()
